import { existsSync } from 'fs';
import { planAndExplain } from '../agent/agent.js';
import { dispatchAgentResult, DispatchedResult } from '../dispatcher/dispatcher.js';
import { executeCommand } from '../exec/runner.js';
import { createPlan, routeIntent } from '../intents/router.js';
import { Intent } from '../intents/types.js';
import { getConfirmation } from '../session/confirmation.js';
import { createMemory, updateMemory } from '../session/memory.js';
import { applyStyleDirective } from '../session/responseStyle.js';
import { AppState, PendingAction } from '../session/state.js';
import { summarize } from '../summarize/index.js';
import { recordAudio, waitForPushToTalk } from '../voice/record.js';
import { streamTranscribe } from '../voice/streamTranscribe.js';
import { transcribe } from '../voice/transcribe.js';
import { speak } from '../voice/tts.js';

import { PlayMode } from '../voice/tts.js';

export interface ChatOptions {
  keepAudio?: boolean;
  player?: string;
  playMode?: PlayMode;
  live?: boolean; // Enable live transcription (default: true)
  silenceMs?: number; // Silence timeout in milliseconds (default: 1000)
}

/**
 * Safely calls speak() without breaking the chat loop if it fails.
 */
async function safeSpeak(
  text: string,
  mute: boolean,
  options: ChatOptions = {}
): Promise<void> {
  if (mute) return;
  try {
    await speak(text, {
      play: !mute,
      playMode: options.playMode || 'stream',
      keepAudio: options.keepAudio,
      player: options.player,
    });
  } catch (error) {
    // Log but don't throw - we want the chat loop to continue
    if (error instanceof Error && error.message.includes('ELEVENLABS_API_KEY')) {
      console.warn('‚ö†Ô∏è  ELEVENLABS_API_KEY not set. Skipping TTS.');
    } else {
      console.error('‚ö†Ô∏è  Failed to speak:', error instanceof Error ? error.message : error);
    }
  }
}

/**
 * Multi-turn chat mode: interactive loop with session memory.
 */
export async function chatMode(
  repoPath: string,
  mute: boolean,
  useAgent: boolean = false,
  options: ChatOptions = {}
): Promise<void> {
  console.log('üé§ DevVoice - Chat Mode');
  console.log(`üìÅ Repository: ${repoPath}`);
  console.log('üí¨ Say "exit" to quit\n');
  
  // Validate repo path
  if (!existsSync(repoPath)) {
    console.error(`‚ùå Repository path does not exist: ${repoPath}`);
    process.exit(1);
  }
  
  const memory = createMemory();
  let currentState: AppState = AppState.LISTENING_FOR_COMMAND;
  let pendingAction: PendingAction | null = null;
  
  while (true) {
    try {
      // State machine: handle different states
      if (currentState === AppState.AWAITING_CONFIRMATION) {
        // Handle confirmation state
        if (!pendingAction) {
          // Should not happen, but reset if it does
          currentState = AppState.LISTENING_FOR_COMMAND;
          continue;
        }

        // Get confirmation (voice or typed)
        // Note: The action details were already shown when entering this state
        const useLiveTranscription = options.live !== false;
        const confirmation = await getConfirmation({
          useVoice: true,
          useLiveTranscription,
          silenceMs: options.silenceMs || 2000,
        });

        if (confirmation === 'yes') {
          // Execute the pending action
          console.log(`\n‚öôÔ∏è  Executing: ${pendingAction.commandTemplate.command} ${pendingAction.commandTemplate.args.join(' ')}`);
          
          try {
            const result = await executeCommand(pendingAction.commandTemplate);
            
            // Update memory
            const summary = summarize(pendingAction.intent, result);
            updateMemory(memory, pendingAction.intent, result, summary);
            
            // Summarize and speak
            console.log(`\nüìä Summary: ${summary}`);
            await safeSpeak(summary, mute, options);
            
            // Show full output if verbose
            if (result.stdout) {
              console.log('\nüìÑ Output:');
              console.log(result.stdout);
            }
            if (result.stderr) {
              console.log('\n‚ö†Ô∏è  Errors:');
              console.log(result.stderr);
            }
          } catch (error) {
            console.error('‚ùå Execution failed:', error);
            const errorText = `Execution failed: ${error instanceof Error ? error.message : 'Unknown error'}`;
            await safeSpeak(errorText, mute, options);
          }
          
          // Clear pending action and return to listening
          pendingAction = null;
          currentState = AppState.LISTENING_FOR_COMMAND;
          console.log('\nüí¨ Anything else? (Press Enter to continue, or say "exit" to quit)');
          continue;
        } else if (confirmation === 'no') {
          // Cancel the action
          const cancelledText = 'Action cancelled.';
          console.log(`\n‚ùå ${cancelledText}`);
          await safeSpeak(cancelledText, mute, options);
          
          // Clear pending action and return to listening
          pendingAction = null;
          currentState = AppState.LISTENING_FOR_COMMAND;
          console.log('\nüí¨ Anything else? (Press Enter to continue, or say "exit" to quit)');
          continue;
        } else {
          // Unclear - reprompt
          const unclearText = "I didn't understand. Please say 'yes' to proceed or 'no' to cancel.";
          console.log(`\n‚ùì ${unclearText}`);
          await safeSpeak(unclearText, mute, options);
          // Stay in AWAITING_CONFIRMATION state
          continue;
        }
      }

      // LISTENING_FOR_COMMAND state
      // Step 1: Wait for push-to-talk
      await waitForPushToTalk();
      
      // Step 2 & 3: Record and transcribe (with live transcription if enabled)
      let transcription: string;
      let audioPath: string | undefined;
      
      const useLiveTranscription = options.live !== false; // Default to true
      
      if (useLiveTranscription) {
        try {
          console.log('üé§ Listening... (Press Enter to stop)');
          const result = await streamTranscribe({
            live: true,
            silenceMs: options.silenceMs || 1000,
            onManualStop: () => false, // Manual stop handled internally
          });
          
          transcription = result.transcript;
          audioPath = result.audioPath;
          
          if (transcription) {
            console.log(`\nüí¨ Heard: "${transcription}"`);
          } else {
            console.log('\n‚ö†Ô∏è  No transcription received');
            continue;
          }
        } catch (error) {
          console.error('‚ùå Streaming transcription failed:', error instanceof Error ? error.message : error);
          // Fall back to batch transcription
          console.log('‚ö†Ô∏è  Falling back to batch transcription...');
          try {
            audioPath = await recordAudio({ durationSeconds: 8 });
            transcription = await transcribe(audioPath);
            console.log(`\nüí¨ Heard: "${transcription}"`);
          } catch (fallbackError) {
            console.error('‚ùå Batch transcription also failed:', fallbackError instanceof Error ? fallbackError.message : fallbackError);
            if (fallbackError instanceof Error && fallbackError.message.includes('ELEVENLABS_API_KEY')) {
              console.log('‚ö†Ô∏è  Continuing chat loop...');
            }
            continue;
          }
        }
      } else {
        // Batch transcription (original behavior)
        console.log('üî¥ Recording... (up to 8 seconds)');
        audioPath = await recordAudio({ durationSeconds: 8 });
        console.log('‚úÖ Recording complete');
        
        console.log('üìù Transcribing...');
        try {
          transcription = await transcribe(audioPath);
          console.log(`\nüí¨ Heard: "${transcription}"`);
        } catch (error) {
          console.error('‚ùå Transcription failed:', error instanceof Error ? error.message : error);
          if (error instanceof Error && error.message.includes('ELEVENLABS_API_KEY')) {
            console.log('‚ö†Ô∏è  Continuing chat loop...');
          }
          continue;
        }
      }
      
      // Apply response style directives (e.g., "be detailed", "short")
      const styleDirective = applyStyleDirective(transcription, memory.responseStyle);
      if (styleDirective.changed) {
        memory.responseStyle = styleDirective.style;
        if (styleDirective.onlyDirective) {
          const ackText = styleDirective.ackText || 'Got it.';
          console.log(`\nüí¨ ${ackText}`);
          await safeSpeak(ackText, mute, options);
          continue;
        }
        if (styleDirective.cleanedText) {
          transcription = styleDirective.cleanedText;
        }
      }

      // Step 4: Plan using AI agent or fallback router
      // Step 4: Plan using AI agent or fallback router and dispatch
      let dispatchedResult: DispatchedResult;
      
      // Check if we're in diagnosis mode and user is responding to a follow-up question
      if (memory.inDiagnosisMode && memory.lastAssistantQuestion) {
        // Simple heuristic: if user response is short and doesn't match a clear new intent, treat as follow-up
        const normalized = transcription.toLowerCase().trim();
        const isNewIntent = normalized.match(/^(run|git|create|commit|help|exit|quit|stop)/i) ||
                           normalized.length > 50; // Long responses are likely new requests
        
        if (!isNewIntent) {
          // Treat as follow-up to diagnosis question
          console.log('üí¨ Continuing diagnosis conversation...');
          
          // For now, provide a simple response acknowledging the follow-up
          // In a full implementation, this could use LLM to continue the conversation
          const followUpResponse = `Thanks for that information. Based on your response, here's what I'd suggest:\n\n`;
          const suggestion = memory.lastRun 
            ? `Since you're dealing with: \`${memory.lastRun.command}\`\n\nTry the steps I mentioned earlier. If you'd like me to run a diagnostic command, just say "yes" or tell me which command to run.`
            : `Review the error output and try the suggested fixes. If you need more help, describe what you've tried.`;
          
          console.log(`\nüí¨ ${followUpResponse}${suggestion}`);
          await safeSpeak(followUpResponse + suggestion, mute, options);
          
          // Clear diagnosis mode after one follow-up to avoid infinite loops
          memory.inDiagnosisMode = false;
          memory.lastAssistantQuestion = undefined;
          
          console.log('\nüí¨ Anything else? (Press Enter to continue, or say "exit" to quit)');
          continue;
        } else {
          // Clear diagnosis mode - user wants to do something new
          memory.inDiagnosisMode = false;
          memory.lastAssistantQuestion = undefined;
        }
      }
      
      if (useAgent && process.env.OPENAI_API_KEY) {
        console.log('ü§ñ Using AI agent for planning...');
        const agentResult = await planAndExplain(transcription, memory);
        console.log(`üìã Agent result received: intent=${agentResult.intent}, confidence=${agentResult.confidence}`);
        
        // Handle low confidence with clarifying question
        if (agentResult.confidence < 0.6 && agentResult.clarifyingQuestion) {
          const questionText = `I'm not sure I understood. ${agentResult.clarifyingQuestion}`;
          console.log(`\n‚ùì ${questionText}`);
          await safeSpeak(questionText, mute, options);
          continue;
        }
        
        // Dispatch agent result
        console.log(`üîÑ Dispatching intent: ${agentResult.intent}`);
        dispatchedResult = await dispatchAgentResult(agentResult, memory, repoPath);
      } else {
        // Fallback to simple router
        const intentResult = routeIntent(transcription);
        const plan = createPlan(intentResult);
        
        // Convert router result to agent result format for dispatcher
        // For CODEBASE_QA, extract query from transcription
        const params = plan.params || {};
        if (plan.intent === Intent.CODEBASE_QA && !params.query) {
          params.query = transcription;
        }
        
        const mockAgentResult = {
          intent: plan.intent,
          params,
          planSteps: [plan.description],
          explanation: undefined,
          confidence: intentResult.confidence,
        };
        
        console.log(`üîÑ Dispatching intent (router): ${plan.intent}`);
        dispatchedResult = await dispatchAgentResult(mockAgentResult, memory, repoPath);
      }
      
      // Step 5: Handle dispatched result
      if (dispatchedResult.type === 'info') {
        // Informational intent - print and speak response immediately
        console.log(`\nüí¨ Response: ${dispatchedResult.responseText}`);
        await safeSpeak(dispatchedResult.responseText, mute, options);
        
        // Handle EXIT intent specially
        if (dispatchedResult.intent === Intent.EXIT) {
          break;
        }
        
        // For other info intents, continue to next iteration (ask for next command)
        console.log('\nüí¨ Anything else? (Press Enter to continue, or say "exit" to quit)');
        console.log(`üîÑ Returning to listening state...`);
        continue;
      }
      
      // Action intent - proceed with execution flow
      console.log(`\nüìã Plan: ${dispatchedResult.plan}`);
      await safeSpeak(`I will ${dispatchedResult.plan.toLowerCase()}`, mute, options);
      
      // Step 6: Handle confirmation requirement
      if (dispatchedResult.requiresConfirmation) {
        // Store pending action and switch to confirmation state
        pendingAction = {
          intent: dispatchedResult.intent,
          description: dispatchedResult.plan,
          commandTemplate: dispatchedResult.commandTemplate,
          params: dispatchedResult.params,
        };
        
        currentState = AppState.AWAITING_CONFIRMATION;
        
        // Show what will be executed
        const confirmationPrompt = `\n‚ö†Ô∏è  This action requires confirmation:\n   ${dispatchedResult.plan}\n   Say "yes" to proceed or "no" to cancel.`;
        console.log(confirmationPrompt);
        await safeSpeak(`This action requires confirmation. ${dispatchedResult.plan}. Say yes to proceed or no to cancel.`, mute, options);
        
        // Continue loop - next iteration will handle AWAITING_CONFIRMATION state
        continue;
      }
      
      // Step 7: Execute (no confirmation needed)
      console.log(`\n‚öôÔ∏è  Executing: ${dispatchedResult.commandTemplate.command} ${dispatchedResult.commandTemplate.args.join(' ')}`);
      const result = await executeCommand(dispatchedResult.commandTemplate);
      
      // Step 8: Update memory
      const summary = summarize(dispatchedResult.intent, result);
      updateMemory(memory, dispatchedResult.intent, result, summary);
      
      // Step 9: Summarize and speak
      console.log(`\nüìä Summary: ${summary}`);
      await safeSpeak(summary, mute, options);
      
      // Show full output if verbose
      if (result.stdout) {
        console.log('\nüìÑ Output:');
        console.log(result.stdout);
      }
      if (result.stderr) {
        console.log('\n‚ö†Ô∏è  Errors:');
        console.log(result.stderr);
      }
      
      // Step 10: Ask for next action
      console.log('\nüí¨ Anything else? (Press Enter to continue, or say "exit" to quit)');
      console.log(`üîÑ Returning to listening state...`);
      
    } catch (error) {
      console.error('‚ùå Error:', error);
      const errorText = `An error occurred: ${error instanceof Error ? error.message : 'Unknown error'}`;
      await safeSpeak(errorText, mute, options);
      // Continue loop instead of exiting
    }
  }
}

function getHelpText(): string {
  return `Available commands: run tests, git status, run lint, run build, create branch, commit, explain failure, details, repeat, help, exit.`;
}
